# 2023_05_12 Arxiv更新论文汇总
今天共有20篇论文


 ## Paper:1




1. Title: Generation of Structurally Realistic Retinal
2. Authors: Sojung Go, Younghoon Ji, Sang Jun Park, Soochahn Lee
3. Affiliation: Sojung Go, Sang Jun Park are with the Department of Ophthalmology, Seoul National University College of Medicine, Seoul National University Bundang Hospital, Seongnam, Korea.
4. Keywords: Fundus images, Fluorescein angiography, Vascular/Fundus image generation, Data augmentation, Vessel segmentation, Artery/Vein classification
5. Urls: Paper link: https://arxiv.org/abs/2305.06813; GitHub: None
6. Summary: 

- (1): 本文旨在提出一种使用扩散模型生成解剖结构准确的视网膜底图像的新技术。
- (2): 过去的方法是使用各种GAN模型进行数据合成，但数据量不足，存在隐私问题和核实问题，成本太高。本文提出的方法采用扩散模型生成动脉/静脉掩模，通过这些掩模生成更逼真的视网膜底图像，并进行了量化评估和临床医生图灵测试，证明了该方法的有效性。
- (3): 本文提出的方法主要分两步：1）无条件生成A / V（动脉/静脉）掩模以建立血管结构; 2）基于A/V掩模条件生成视网膜底图像。为解决第一步中掩模的稀疏性问题，我们引入了一种修改后的扩散模型损失函数。
- (4): 本文提出的方法可以生成具有解剖结构准确的血管结构的高质量图像，并能根据扩散模型的强度创建各种多样性的图像。通过在血管分割和动脉/静脉分类的数据增强上使用我们的方法，我们展示了该方法具有的性能提升。此外，我们通过临床专家图灵测试展示了我们生成的图像与真实图像难以区分。我们相信该方法可以用于构建独立数据集而不涉及患者隐私。
- (5): 视网膜底图像为诊断多种眼部疾病提供了重要的依据，但由于从事机器学习任务需求和隐私问题，构造用于训练的标记真实数据集的限制使其成为一项挑战。本文旨在提出一种方法，以使用生成模型合成大量高质量的视网膜底图像，克服数据集限制的挑战，并能够更好地支持血管分割和动脉/ 静脉分类等任务的研究。




 ## Paper:2




1. Title: A General-Purpose Multilingual Document Encoder (通用多语种文档编码器)

2. Authors: Onur Galoğlu, Robert Litschko, Goran Glavaš

3. Affiliation: Independent Researcher (独立研究者)

4. Keywords: Multilingual transformers, document-level tasks, hierarchical transformer model, cross-lingual contrastive objective, cross-lingual document retrieval (多语种transformers，文档级任务，分层transformer模型，跨语言对比目标，跨语言文档检索)

5. Urls: Paper: https://arxiv.org/abs/2305.07016  Github: https://github.com/ogaloglu/pre-training-multilingual-document-encoders

6. Summary:

- (1):该论文的研究背景为多语种NLP，特别是跨语言转移NLP模型的现状。

- (2):过去的方法主要集中在从MMTs中提取双语文本并诱导双语文档嵌入向量，然而很少有研究将MMTs用于训练能够对有监督和无监督文档级任务都有用的通用（大规模）多语种文档编码器。本文分层使用transformer模型（HMDE）对文档编码器进行预训练，以线上可获取的Wikipedia为数据源进行跨语言对比训练，最终成功通过该方法进行跨语言文档检索。

- (3):本文中的研究方法是使用预训练好的多语种句子编码器来生产单句子中间层的上下文表示，然后使用这些表示来训练文档编码器。在训练阶段，通过使用Wikipedia上可比较的文档来创建训练数据，并利用Wikipedia的类别层次用于创造困难样例。

- (4):本文中的方法在两个跨语言文档级任务（跨语言主题文档分类和跨语言文档检索）上均表现更出色，且在文档级预训练中，HMDE能够成功将转移至该语言中。

- (5):本文的动机是通过设计一个通用的、高效的文档编码器，在多语种语言转移NLP模型方面进行改进并取得更好的效果。




 ## Paper:3




1. Title: Segment and Track Anything (中文翻译：分割与跟踪任何物体)

2. Authors: Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin Yang, Wenguan Wang, Yi Yang

3. Affiliation: ReLER, CCAI, Zhejiang University (中文翻译：浙江大学-智能计算与人工智能研究中心)

4. Keywords: video segmentation, object tracking, interactive methods, multimodal interaction, computer vision

5. Urls: Paper: arXiv:2305.06558v1, Github: https://github.com/z-x-yang/Segment-and-Track-Anything

6. Summary:

- (1): 本文以视频分割与物体跟踪为研究课题，涉及各个领域的应用，例如自动驾驶、医疗成像、增强现实、生物分析等。

- (2): 过去的方法中，自动视频分割、半自动视频分割、基于笔画或点击的交互式视频分割及基于语言的视频分割各自存在问题，而本文使用多模态交互方法可以满足各个领域的不同需求，结合"Segment Anything Model"（交互式关键帧分割模型）和最新提出的"AOT-based tracking model"（VOT 2022 挑战第一名的模型）来实现视频物体跟踪，同时整合文本交互模式使得适合更多的场景应用。

- (3): 本文提出了"Segment and Track Anything"框架，该框架融合了"SAM"和"DeAOT"模型实现了高质量的物体分割与跟踪，同时通过"Grouding-DINO"模块实现文本交互，拓展应用场景。

- (4): 本文在 DAVIS-2016 Val (92.0%) 及 DAVIS-2017 Test (79.2%) 数据集上的表现已经极其优异，并且该框架也适用于多种领域的物体分割与追踪任务，表现支持了本文作者的研究目标。

- (5): 本文所述研究背景为视频分割及对象跟踪在各个领域应用流行的背景下，当前现有的解决方法存在问题，因此需要一种新的、更适用于各种情况的框架。




 ## Paper:4




1. Title: Leveraging Prior Knowledge for Blind Super-Resolution with Pre-Trained Diffusion Models

2. Authors: Zhendong Wang, Jianlong Fu, Xiaochun Cao, Haojie Li, Ying Shan

3. Affiliation: 西安电子科技大学

4. Keywords: Blind super-resolution, diffusion models, prior knowledge, controllable feature wrapping

5. Urls: Paper: https://arxiv.org/abs/2110.08632, Github: None

6. Summary:

- (1): 本文研究的是盲超分辨率问题，利用预训练的扩散模型中所包含的先前知识进行盲超分辨率重建。

- (2): 以往的方法常常需要使用显式的先验或者特定的样本来进行超分辨率处理，而且实现效果有限。与这些方法不同，本文提出的方法利用扩散模型中的先验知识，通过微调既有的扩散模型来实现超分辨率恢复。为了解决扩散模型的随机性带来的细节损失问题，本文引入了一个可控的特征包裹模块，用于在保持高保真度的前提下实现高质量的超分辨率重建。这个方法的动机合理，创新性明显，值得进一步的研究和应用。

- (3): 本文主要的贡献是提出了一种基于扩散模型的盲超分辨率方法，该方法利用时间感知编码器来对预训练的扩散模型进行微调，以便更好地适应低分辨率的输入图像。此外，为了在 quality 和 fidelity 之间平衡，我们采用了可控特征包裹模块，使用户可以在推理过程中通过调整一个标量值来平衡重建质量和细节还原情况。为了克服预训练扩散模型尺寸限制的固有性问题，本文还提出了一种渐进聚合采样策略，以便在任何尺寸的情况下进行适应。

- (4): 实验结果表明，本文所提出的方法在各项性能指标上都显著优于现有的超分辨率方法，其超分辨率效果接近理论极限，说明方法具有较好的性能和扩展性。

- (5): 本文的研究动因是要解决盲超分辨率问题，提高超分辨率的效果和精度。与现有的方法不同，本文提出了一种基于预训练扩散模型的盲超分辨率方法，该方法具有更高的性能和更好的可扩展性，完美地解决了现有解决方案中存在的问题。




 ## Paper:5




1. Title: Simple Token-Level Conﬁdence Improves Caption Correctness (使用简单的令牌级置信度来提高图像标题的正确性)

2. Authors: Suzanne Petryk, Spencer Whitehead, Joseph E. Gonzalez, Trevor Darrell, Anna Rohrbach, Marcus Rohrbach

3. Affiliation: 1 UC Berkeley（加利福尼亚大学伯克利分校）; 2 Meta AI

4. Keywords: deeplearning, vision-language understanding, image captioning, Token-Level Conﬁdence

5. Urls: Paper: https://arxiv.org/abs/2305.07021v1; Github: None

6. Summary:

- (1): 进行视觉和语言模型的图像文本匹配（ITM）和评估标题的准确度是一项重要的任务，因为它可以帮助我们理解视觉和语言之间的关系。然而，目前的方法无法完全判断微观细节的正确性，所以长期以来一直存在一些问题（如大量虚构物体等）。 

- (2): 相应地，作者提出使用令牌级置信度（TLC）来解决这个问题，并通过微调视觉语言模型，输入图像和标题所提供的信息，来精确地评估图像标题的一致性。TLC一方面极为简单，另一方面又是一种相当高效的方法。与预训练模型的序列级分数比较，在SOV-Probes中，使用代数置信度对动词的理解在准确性方面提高了10%；在具有额外组知识的图像推理测试集”Winoground“中，使用代数置信度对图像和组的准确度比现有的先进水平提高了37％和9％。当提供合适的数据时，学习的置信度评估方法提高了很多，并且可以使MS COCO Captions数据集中物体虚构率下降30％，并创造新的先进水平。

- (3): 根据令牌级置信度可以对每个词的一致性进行评估，并支持微观分析。它使我们能够准确地跟踪序列中每个标记的一致性并提高模型对细节的敏感性。

- (4):在SOV-Probes测试集中已经证明，在动词理解和图像组合方面精度有所提高。 在具有“Winoground”组知识的图像推理测试集中，代数置信度比现有的技术提高37％和9％。在MS COCO Captions数据集中，使用一个学习置信度评估器，能够使物体幻觉减少30％，并且创建了新的先进水平。

- (5):研究背景是当前视觉语言理解模型无法通过细节区分图像及其与标签描述之间的关系。此研究的目标是基于单词级别对描述图像的标题进行准确评估。




 ## Paper:6




1. Title: FACTKG: Fact Verification via Reasoning on Knowledge Graphs 

2. Authors: Jiho Kim, Sungjin Park, Yeonsu Kwon, Yohan Jo, James Thorne, Edward Choi 

3. Affiliation: Jiho Kim, Sungjin Park, Yeonsu Kwon, James Thorne, Edward Choi: KAIST, Yohan Jo: Amazon 

4. Keywords: Knowledge Graph, Fact Verification, Natural Language Processing, Machine Learning, Reasoning 

5. URLs: Paper: https://arxiv.org/abs/2305.06590, Github: None 

6. Summary: 
- (1): 该研究的背景是近年来随着网络信息的爆炸性增长， Fact Verification （事实验证）面临越来越多的挑战，KG 作为一种在各个领域广泛使用的大规模数据形式，可以作为事实验证的重要知识来源。 
- (2): 过去的方法主要根据文本证据进行分类，验证决策的可靠性存在挑战。该方法通过 KG 中的节点和关联将常见的推理类型分类，可以提供更可靠的推理和具有更广泛适用性的方式。 
- (3): 该论文提出了一种基于 KG 的事实验证研究方法，通过开发 FACTKG 数据集并分析其推理类型，开启了 KG 在事实验证中的应用。 
- (4): 该方法在 FACTKG 数据集上进行了测试，分别进行单跳，合取，存在，多跳和否定五种类型的推理，提高了 KG 在事实验证中的实用性和可靠性。 
- (5): 该研究的动机是 KG 作为一种灵活且具有广泛适用性的数据形式在事实验证中具有潜力，并致力于为研究提供一个新的KG推理数据集 FACTKG，以便更好地了解在事实验证中如何推理 KG。




 ## Paper:7




1. Title: Chain-of-Dictionary Prompting Elicits Translation (用词典链引导语言模型进行翻译)

2. Authors: Hongyuan Lu, Haoyang Huang, Dongdong Zhang, Haoran Yang, Wai Lam, Furu Wei

3. Affiliation: 香港中文大学 (The Chinese University of Hong Kong)

4. Keywords: large language models, machine translation, multilingual, dictionary, low-resource

5. Urls: 
- Paper: https://arxiv.org/abs/2305.06575v1
- Github: None

6. Summary:
- (1): 该文研究使用词典链增强对贫资源语言机器翻译的翻译能力
- (2): 文章介绍了过去的方法以及存在的问题，并提出了新的方法。新方法利用词典链来引导语言模型进行翻译，并对模型的性能进行了实验验证。
- (3): 该文提出了一种用词典链来引导语言模型进行翻译的方法，通过加入提示信息的形式，在标准翻译提示语句的前面加入表示特定单词多语言翻译的词典链，以提高语言模型的翻译能力。
- (4): 在FLORES-200基准测试中，该方法在低资源翻译方面取得了显著的改进。该方法可以显著提高ChatGPT的MNMT性能，提升幅度高达13倍，即ChrF++得分从3.08提升到42.63（英语到塞尔维亚文的西里尔字母），证明了该方法的有效性。
- (5): 该研究的动因是提高low-resource语言的机器翻译能力，旨在利用先验知识降低过度依赖平行数据的现象。




 ## Paper:8




1. Title: Analyzing Bias in Diffusion-based Face Generation Models (分析基于扩散的人脸生成模型中的偏差问题)

2. Authors: Malsha V. Perera, Vishal M. Patel

3. Affiliation: Malsha V. Perera 属于约翰霍普金斯大学 (Johns Hopkins University)

4. Keywords: diffusion models, bias, face generation, generative adversarial networks (GAN), training dataset

5. Urls: 
- Paper link: arXiv:2305.06402v1 [cs.CV] 10 May 2023 
- Github link: None

6. Summary: 

- (1): 本文的研究背景在于，生成模型在实际应用中越来越受到重视，但是这些模型也可能扩大现有的偏见问题，尤其是人脸生成任务，偏见问题可能对社会产生严重后果，因此需要研究扩散模型在人脸生成任务中存在的偏差问题。

- (2): 本文主要研究扩散模型在人脸生成任务中的偏差问题，并比较其与生成对抗网络 (GAN) 的表现。此外，还分析了训练数据集大小对扩散模型产生的偏差问题的影响，以及各个属性类别在各种生成模型中的感知质量差异。

- (3): 本文的研究方法包括对基于扩散的人脸生成模型指定属性的数据进行训练、对生成数据的自然属性进行定量评估、通过交叉验证和属性子集对比来评估扩散模型和GAN模型的表现、比较数据集大小对生成效果的影响。

- (4): 在多组实验中，本文发现了扩散模型存在着训练数据分布偏差的问题，这种偏差很大程度上受到训练数据集大小的影响，而GAN模型在训练数据集合适、数据采样平衡的情况下存在的偏差较少。同时，在对比实验中，GAN模型同样表现出了优异的性能。

- (5): 本文的研究动机在于探究扩散模型在人脸生成任务中的偏差问题，以便了解偏差的根本原因，并制定相关策略以减小其影响，最终为各项实际应用营造公正、无偏见的环境。




 ## Paper:9




1. Title: InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language

2. Authors: Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue Yang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng Dai, Yu Qiao

3. Affiliation: 上海人工智能实验室

4. Keywords: deeplearning, chatbots, visual tasks, vision-centric tasks

5. Urls: https://github.com/OpenGVLab/InternGPT, Github: None

6. Summary: 

- (1):本文研究的背景是在视觉中心任务方面，如何提高机器对视觉信息的理解和反应能力。

- (2):过去的方法是使用特定的视觉基础模型进行逐一处理，但其受限于标记数据的可用性和质量，且适用性有限。本文提出的方法结合了chatbot和指向动作，即嵌入指针运动指令的chatbot，通过指针运动提供更灵活和精确的控制视觉内容的方式。相较于纯语言的交互方式，使用指针指令的交互方式在复杂视觉场景下表现更好。本文定义的框架被称为InternGPT，其中Intern表示交互，GPT表示chatbot。此外，本文引入辅助控制机制以提高LLM的控制能力，并使用名为“Husky”的大型视觉语言模型进行多模态对话。

- (3):本文方法是结合chatbot和指针动作来控制视觉内容。

- (4):本文方法在视觉中心任务中取得一定的成功，特别是在复杂的场景中，指针指令作为交互方式比纯语言方式更好。本文引入的辅助控制机制和大型视觉语言模型可以提高chatbot的效率和准确性。

- (5):本文的动机是探索一种新的视觉交互方式，结合chatbot和指针动作，使得视觉控制更加灵活和高效。同时，也希望为未来的交互式视觉系统提供新的思路和方向。




 ## Paper:10




1. Title: ChatGPT-Like Large-Scale Foundation Models for AI 1.0 to AI 2.0 

2. Authors: Huan Wang, Wei Qian, Dongrui Wu, Lingyang Zu, Qing Wang

3. Affiliation: 清华大学自动化系, Tsinghua University Department of Automation

4. Keywords: Prognostics and Health Management, Fault Diagnosis, Large-Scale Foundation Model, Representation Learning

5. URLs: Paper: None, Github code link: None

6. Summary:

- (1): 该文章的研究背景是近年来人工智能在产业物联网和大数据背景下，基于深度学习的PHM技术得以迅速发展，旨在通过自动化数据分析、设备状态监测、健康管理，实现工业设备维护的智能化和自动化。

- (2): 传统的PHM技术主要基于机器学习和深度学习实现智能状态识别和决策。然而，机器学习受到特征工程的限制，深度学习的学习能力和计算效率存在问题。本文提出了一种基于ChatGPT-Like的大规模基础模型，具有多模态数据融合、无监督表示学习和跨领域知识等优势，能够克服现有模型的不足。

- (3): 本文的研究方法是通过构建大规模跨领域知识基础模型，实现PHM数据的自动化分析和设备状态监测，并探索了该模型在多模态融合、无监督表示学习等方面的应用。

- (4): 本文所提出的基础模型在多个PHM应用任务中取得了很好的性能表现，包括故障诊断、异常检测和健康监测等方面，且能够充分支持其目标。

- (5): 该文章的研究动机在于发现现有模型存在一些限制和不足，需要构建跨领域基础模型来实现智能化PHM技术的自动化和大规模化。




 ## Paper:11




1. Title: Enabling Programming Thinking in Large Language Models Toward Code Generation (中文翻译：在大型语言模型中实现代码生成的编程思维)

2. Authors: Jia Li, Ge Li, Yongmin Li, Zhi Jin

3. Affiliation: Key Lab of High Confidence Software Technology, MoE (Peking University) (中文翻译：北京大学高可信软件技术教育部重点实验室)

4. Keywords: Large Language Models, code generation, programming thinking, deep learning, natural language processing, computer vision (中文翻译：大型语言模型，代码生成，编程思维，深度学习，自然语言处理，计算机视觉)

5. Urls: Paper - https://arxiv.org/abs/2305.06599v1, Github - None

6. Summary: 
- (1):本文旨在探讨如何在大型语言模型中实现编程思维来提高代码生成的性能。
- (2):过去的研究使用大型语言模型直接完成从需求到代码的生成，但缺乏编程思维，从而导致性能有限。本文提出了一种名为TIP的方法，通过分解代码生成过程，让大型语言模型逐步进行编程思维分析和实现。该方法首先生成一个高层次的代码草图，用编程逻辑提供一个高层次的解决过程，但省略了实现细节。然后，TIP使用特定的编程语言将草图实现成程序。在三个公共基准测试中（HumanEval、MBPP和MBCPP），TIP在Pass@1、Pass@3和Pass@5方面的表现均比其它研究方法好，人类评估表明TIP的正确性、代码质量和可维护性都优于ChatGPT。本文还探讨了TIP和其它后处理方法（例如CodeT）之间的互补性。
- (3):本文提出的TIP方法将代码生成分解为两个过程，让大型语言模型可以逐步进行编程想法的分析和实现，提高代码生成的性能。
- (4):本文使用三个公共基准测试对所提出的TIP方法进行了说明，并表明该方法对不同的大型语言模型都有很好的效果，在正确性、代码质量和可维护性方面的表现也都比其它研究方法好。
- (5):本文的研究动机是使用大型语言模型进行代码生成时，现有研究常常缺乏编程思维，从而导致精度有限。本文提出了一种新的方法来利用大型语言模型生成更好的代码。




 ## Paper:12




1. Title: Musketeer (All for One, and One for All)

2. Authors: Zhaoyang Zhang, Yantao Shen, Kunyu Shi, Zhaowei Cai, Jun Fang, Siqi Deng, Hao Yang, Davide Modolo, Zhuowen Tu, Stefano Soatto

3. Affiliation: 研究者Zhaoyang Zhang的Affiliation是"The Chinese University of Hong Kong"

4. Keywords: Vision-language model, multi-task learning, task explanation prompts

5. Urls: Arxiv: https://arxiv.org/abs/2305.07019  

6. Summary:

- (1):本文的研究背景是多任务学习中任务之间共享的问题。

- (2):过去的方法通过在通用模型上使用各个任务的特定头部来解决这个问题，但是这种方法不利于任务共享和模型的可扩展性。新方法使用Task Explanation Prompts (TEPs)来达到任务之间无干扰状态，使得模型更容易关注任务之间的共同之处，也使得模型任务间参数共享更加顺畅。

- (3):本文提出了一种名为 Musketeer 的通用语言视觉模型，可以在不需要任务特定头的情况下进行多任务学习。该模型使用 Task Explanation Prompts (TEPs)，对于输入数据，输出空间，数据集等进行更好的描述和处理。通用模型的表示学习和任务特定头分开，在联合学习过程中相互独立。同时，使用TEPs使得模型可以轻松学习处理多任务，但不同时影响其他任务。

- (4):本文所提出的 Musketeer 模型在多个视觉任务中均表现出良好的性能。特别地，在CUB, COCO, Flickr30K, Visual Genome, and SBU测试集中也达到了优异的结果。这些结果表明 Musketeer 可以作为一个通用模型在多任务情况下具有良好的扩展性和性能。

- (5):文章的原因是构建一个全能的语言视觉模型来支持任务之间的共享和任务参数之间的共享，让模型具有更好的可扩展性和通用性，实现各任务均有良好的性能表现。




 ## Paper:13




1. Title: Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator (扩散模型中的零文本指导暗地里是个漫画风格创作者)

2. Authors: Jing Zhao, Heliang Zheng, Chaoyue Wang, Long Lan, Wanrong Huang, Wenjing Yang

3. Affiliation: National University of Defense Technology (中国国防科技大学)

4. Keywords: Classifier-free guidance, Cartoonization, Null-text guidance, Diffusion models (无分类器指导，漫画化，零文本指导，扩散模型)

5. Urls: Paper: arXiv:2305.06710v1, Github: https://nulltextforcartoon.github.io/

6. Summary:

- (1): 该文章旨在揭示无分类器指导在扩散模型下，零文本指导实际上可以用于漫画风格画作品的生成。

- (2): 以往的方法主要依靠分类器指导或使用随机高斯噪声，而且这些方法存在一些问题，特别是分类器指导需要额外训练，另外很难实现对生成效果的控制。与其不同的是，该文章使用一种新的噪声扰动方法来达到目的，避免了使用分类器指导的限制，并且提高了生成的选择性和控制性。

- (3): 该文章提出了两种扰动方法，即Rollback Disturbance（Back-D）和Image Disturbance（Image-D）。Back-D通过改变零文本噪声图像的噪声等级，通过取代 𝑥𝑡 with 𝑥𝑡+Δ𝑡 实现漫画化的作用。Image-D通过将𝑥𝑡定义为一个干净的输入图像来产生高保真度、多样性的漫画图像，进一步提高了细节的融合。

- (4): 该方法的实验结果表明，在不需要分类器指导的情况下，该方法可以生成高质量的漫画画作，同时该方法运行速度较快，容易实现。漫画效果的度量标准TV Loss和LPIPS也与真实漫画类似。

- (5): 该方法的提出为扩散模型的零文本噪声控制提供了新的思路，并且可以用于诸如漫画化、图像风格迁移等任务。




 ## Paper:14




1. Title: An Inverse Scaling Law for CLIP Training （CLIP 训练的反比例缩放定律）

2. Authors: Xianhang Li, Zeyu Wang, Cihang Xie

3. Affiliation: UC Santa Cruz（圣塔克鲁兹加州大学）

4. Keywords: CLIP, computer vision, natural language processing, deep learning, training cost（CLIP，计算机视觉，自然语言处理，深度学习，训练成本）

5. Urls: Paper: arXiv:2305.07017v1  [cs.CV]  11 May 2023; Github: https://github.com/UCSC-VLAA/CLIPA.

6. Summary:

- (1): 本文研究的背景是深度学习模型的训练成本高，导致许多研究者不能参与该领域，限制了模型的复现和发展。（The background of this article is that the training cost of deep learning models is high, which prevents many researchers from participating in this field, limiting the reproducibility and development of models.）

- (2): 以往的方法过于依赖大规模训练和完整的数据集，难以在学术资源的限制下进行研究，降低了研究者的参与度。本文提出一种反比例缩放定律，能够使得同时运行大型图像/文本编码器和较短的序列长度的训练速度加快。这种方法可以在纽约州立大学学院和圣塔克鲁兹加州大学的学术资源下成功地训练 CLIP 模型。（The previous methods relied too much on large-scale training datasets and complete data, making it difficult to conduct research under academic resource constraints and reducing researchers' involvement. This paper proposes an inverse scaling law that can speed up training by simultaneously running large image/text encoders and shorter sequence lengths. This approach can successfully train CLIP models on academic resources at State University of New York and UC Santa Cruz. ）

- (3): 本文的研究方法主要是分析序列长度和计算成本的关系，并且介绍了一种针对序列长度削减的特定策略，能够大幅度减少 CLIP 训练的计算成本，从而将 CLIP 的性能扩展到更多学术资源。 （The main research method of this paper is to analyze the relationship between sequence length and computational cost, and introduce a specific strategy for reducing sequence length, which can greatly reduce the computational cost of CLIP training and extend the performance of CLIP to more academic resources.）

- (4): 本文的研究结果表明，运用规模更大的图像/文本编码器和较短的序列长度训练可以提高模型的性能和训练速度，并且这种方法可以在较低的计算成本下成功训练 CLIP 模型，从而获得了63.2％、67.8％和69.3％的 zero-shot top-1 ImageNet 精度。（The research results of this paper indicate that training with larger image/text encoders and shorter sequence lengths can improve model performance and training speed. This approach can successfully train CLIP models at a lower computational cost, achieving zero-shot top-1 ImageNet accuracies of 63.2%, 67.8%, and 69.3%.)

- (5): 本文的研究动机在于解决现有 CLIP 训练成本高的问题，扩大 CLIP 的应用范围，提高计算机视觉和自然语言处理领域的研究者的参与率。（The motivation of this paper is to solve the problem of high training costs in existing CLIP models, expand the application scope of CLIP, and increase the participation rate of researchers in computer vision and natural language processing fields.）




 ## Paper:15




1. Title: Detecting ChatGPT Imposters with A Single Question

2. Authors: Hong Wang, Xuan Luo, Weizhi Wang, Xifeng Yan

3. Affiliation: University of California Santa Barbara (Hong Wang)

4. Keywords: Large Language Model, ChatGPT, Bot Detection, Natural Language Processing, Conversation Simulation

5. Urls: Paper url: https://arxiv.org/abs/2305.06424v1 
Github: https://github.com/hongwang600/FLAIR (code available)

6. Summary:

- (1): The article discusses the potential misuse of large language models, such as ChatGPT, for malicious purposes, and highlights the need to develop methods for detecting whether the party involved in a conversation is a human or a bot.

- (2): The paper proposes a framework called "FLAIR" to detect conversational bots in an online manner, using a single question scenario to effectively differentiate human users from bots. They compare their approach against traditional methods and show their approach's superior performance in detecting bots, even in adversarial settings where bots try to imitate human responses.

- (3): The paper uses a two-stage approach for bot detection, where they first determine the user's level of conversational capability and then ask a customized question based on the user's level. The questions are categorized as easy or difficult for humans or bots, targeting areas such as counting, substitution, noise filtering, and computation. They show the effectiveness of these categories of questions and how they can be used to differentiate humans from bots in an online environment.

- (4): The authors evaluate their approach on a large-scale, real-world dataset and show that their approach achieves a high detection accuracy of 98.96%. They also analyze the robustness of their approach in adversarial settings and show that their approach is more robust than traditional methods. The performance achieved supports their goal of developing an effective bot detection method.

- (5): The motivation for this research is to develop methods for detecting conversational bots in an online environment, to prevent the misuse of large language models for malicious purposes and to ensure that online service providers are serving real users.




 ## Paper:16




1. Title: Combo of Thinking and Observing for Outside-Knowledge VQA（结合思考和观察的面向外部知识的 VQA）

2. Authors: Qingyi Si, Yuchen Mo, Zheng Lin, Huishan Ji, Weiping Wang

3. Affiliation: 中国科学院信息工程研究所、中国科学院大学网络空间安全学院

4. Keywords: outside-knowledge VQA, cross-modality space, natural-language space, multimodal encoder, textual encoder

5. Urls: https://arxiv.org/abs/2305.06407, https://github.com/PhoebusSi/Thinking-while-Observing

6. Summary:

- (1): 本文研究的背景是外部知识的视觉问题回答任务，这为系统回答具有挑战性和多样化的问题提供了更多的开放性。

- (2): 既有的视觉问题回答任务侧重于融合图像和文字信息，但往往无法涵盖更广泛的自然语言空间中的知识；有些方法将图像转化为文本，完全抛弃了视觉特征。本文综合前人方法并在此基础上提出新的方法，即将交叉模态空间限定在自然语言空间中，将视觉特征直接保留下来，并且仍然从自然语言空间中获益。本文采用了包含多模态编码器、文本编码器和回答解码器的新框架，以引入更多类型的显式和隐式多模态和文本知识。相关实验验证了本文方法的优越性，超过现有最优方法的准确率提高了 6.17%，并进行了全面的组件削减，系统地研究了各种类型知识的作用。

- (3): 本文提出了一种新方法，即将交叉模态空间限定在自然语言空间内，以保留视觉特征并融合更多类型的显式和隐式多模态和文本知识。

- (4): 本文所采用的方法在外部知识的视觉问题回答任务上表现出了出色的性能，超出现有最优方法的准确率提高了 6.17%。

- (5): 本文的动机在于提高外部知识的视觉问题回答任务的性能，为回答更加复杂的问题提供更多可供利用的开放式多模态和文本知识。




 ## Paper:17




1. Title: EfﬁcientViT: Memory Efﬁcient Vision Transformer with Cascaded Group Attention

2. Authors: Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, Yixuan Yuan

3. Affiliation: 第一作者：The Chinese University of Hong Kong

4. Keywords: deeplearning, ML, vision transformer, memory efficiency, group attention

5. Urls: Paper: arXiv:2305.07027v1; Github:https://github.com/Microsoft/EfficientVisionTransformer

6. Summary: 

- (1):本文研究的背景为视觉Transformer（Vision Transformers, ViTs）在高性能方面取得了不俗的成绩，但其模型大小和计算开销增加导致其在实时应用方面受到限制。

- (2):过去的方法主要关注参数或Flops的缩减，但并不能反映模型实际推理速度。本文提出一个新的模块，实现了单内存绑定的转换器，以提高内存效率。同时，作者发现注意力映射在各个头部之间具有高相似性并存在计算冗余。作者设计了一个级联组注意力模块，使不同的注意力头部接收带有不同参数分割点的全特征，从而既可以节省计算成本，还可以提高注意力多样性。

- (3):本文采用了实验和分析的方法来研究影响转换器模型推理速度的因素，设计了一个新的模块来提高内存效率和减少计算冗余。

- (4):作者在ImageNet-1K数据集上进行了大量实验，EfﬁcientViT-M5模型在准确率上优于MobileNetV3-Large模型，同时在Nvidia V100 GPU和Intel Xeon CPU上分别获得40.4%和45.2%的较高吞吐量。而在GPU/CPU上，相较于最近的MobileViT-XXS模型，EfﬁcientViT-M2模型运行速度分别更快了5.8倍和3.7倍，转换为ONNX格式后更快了7.4倍，实现了速度和准确度的良好平衡。

- (5):本文的研究动机在于探索视觉Transformer模型的高速原则，并提出一种新的高效转换器模型，可以在实时应用中获得较高的推理速度。




 ## Paper:18




1. Title: SparseGNV: Generating Novel Views of Indoor Scenes with Sparse Input Views (室内场景中的稀疏输入生成新视角)

2. Authors: Weihao Cheng, Yan-Pei Cao, Ying Shan 

3. Affiliation: ARC Lab, Tencent PCG (腾讯ARC实验室)

4. Keywords: deeplearning, computer vision, novel view synthesis, sparse input views, neural point cloud 

5. Urls: Paper: https://arxiv.org/abs/2305.07024 ; Github: https://github.com/xt4d/SparseGNV 

6. Summary: 

- (1): 该文研究如何利用稀疏的输入视角生成室内场景中的新视角。

- (2): 过去的方法通常需要大量从各个角度得到的扫描信息，但很难获取全部区域的信息。 稀疏视角的生成方法对于解决这个问题尤为重要，例如最近流行的基于神经辐射场的算法 (NeRF)。然而，由于缺乏图像生成能力，这些方法很难生成大量未见过的区域。另一类基于transformer的方法可以从2D观察中学习场地的表示，并根据新视点条件生成图像。然而，无明确3D表示的缺陷使这些方法难以从未结构化的潜在空间合成视觉细节。本文提出的SparseGNV框架，将3D结构和图像生成模型结合在一起，实现了生成视角和保持场景一致性的联合功能。

- (3): 本文提出了一个包含三个模块的生成新视角的模型。第一个模块将神经点云作为底层几何结构，为生成目标视角提供上下文信息和引导。第二个模块将场景上下文和引导映射到共享的潜在空间，自回归地解码目标视角。第三个模块将解码的令牌还原为目标视角的图像。SparseGNV可以通过大型室内场景数据集进行训练，以学习可推广的prior。一旦训练完毕，它可以在没有场景特定优化的情况下，在新场景的观测视角和目标视点之间有效地生成novel views。

- (4): 本文评估SparseGNV在真实世界和合成室内场景上的表现，并表明它在不使用神经辐射场或条件图像生成的情况下，都能在视觉真实性和一致性方面超越现有技术，证明了本文所提方法的有效性。

- (5): 本文的主要动机是开发一种生成稀疏输入视图中新视角的方法，并在保持视图一致性和视觉真实性的同时，减少数据的采集负担。




 ## Paper:19




1. Title: Autonomous GIS: the next-generation AI-powered GIS (自主GIS：下一代基于人工智能的GIS)

2. Authors: Yuchuan Cai, Xiaoling Sun, Tongguang Ren, Yan Huang

3. Affiliation: Yuchuan Cai, Xiaoling Sun, and Yan Huang are affiliated with School of Resource and Environmental Sciences, Wuhan University, Wuhan, Hubei Province, China

4. Keywords: Autonomous Agent, GIS, Artificial Intelligence, Spatial Analysis, Large Language Models, ChatGPT

5. Urls: Paper: None Github code: https://github.com/gladcolor/LLM-Geo

6. Summary:

- (1): This article focuses on the integration of Large Language Models (LLMs) and Geographical Information Systems (GIS) to create an autonomous GIS.

- (2): The past methods were limited and lacked a decision-making core that could adapt to changing conditions and make informed decisions. The proposed approach leverages LLMs' abilities in natural language understanding, reasoning, and coding for addressing spatial problems with automatic spatial data collection, analysis, and visualization.

- (3): The proposed methodology involves developing a proof-of-concept prototype called LLM-Geo, capable of conducting spatial analysis in an autonomous manner. LLM-Geo receives tasks (spatial problems/questions) from users and generates a solution graph (geoprocessing workflow) by decomposing the task into successive connected data operations as a directed acyclic graph. Each operation is a function implemented by the LLM, which is GPT-4 in this study.

- (4): Two case studies are used to verify LLM-Geo's ability, demonstrating that the integration of LLMs into GIS has the potential to automate complex spatial analysis tasks and make GIS technology more accessible to individuals without GIS backgrounds. LLM-Geo significantly reduces manual operation time and serves as a potential path towards the next generation of AI-powered autonomous GIS.

- (5): The motivation for this research is to revolutionize the field of GIS by automating complex spatial analysis tasks and making GIS technology more accessible to a broader audience. The authors advocate for more effort dedicated to research and development of autonomous GIS to achieve five autonomous goals, including self-generating, self-organizing, self-verifying, self-executing, and self-growing.




 ## Paper:20




1. Title: Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition
2. Authors: Anis Koubaa, Basit Qureshi, Adel Ammar, Zahid Khan, Wadii Boulila, Lahouari Ghouti
3. Affiliation: Prince Sultan University, Saudi Arabia (阿拉伯王子苏丹大学)
4. Keywords: ChatGPT, GPT-4, GPT-3.5, GPT Performance, GPT Limitations, OpenAI, NLP, Computer Programming
5. Url: arXiv:2305.06934v1 , Github: None
6. Summary: 
- (1): 本文针对ChatGPT在编程评估和问题解决领域的表现能力展开研究;
- (2): 本文介绍了大型语言模型(Large Language Models)，并指出ChatGPT在这种类型的模型中是引人注目的。然而，在编程评估和问题解决领域，过去的研究方法存在问题。本文在这一背景下，使用IEEExtreme Programming Challenge作为对比 benchmark。 这项全球性、年度性的国际编程竞赛吸引了全球的专业程序员参与，要求具备高水平的问题解决和编程技巧。文章描述了ChatGPT在IEEExtreme programming问题集上进行评估的结果，并且发现人类程序员在编程问题解决方面的竞争优势仍然存在。 
- (3): 本文针对ChatGPT在编程评估和问题解决领域的表现能力展开研究。使用了IEEExtreme Programming Challenge作为benchmark，评估ChatGPT在编程问题集中的表现。同时，对ChatGPT的局限性进行了分析，并提出了对其未来发展的建议。
- (4): 本文的研究表明，ChatGPT在处理编程问题时，与人类程序员的表现相比存在明显的劣势。根据对一系列IEEExtreme programming竞赛问题的评估，以Python、Java和C++三种主要编程语言为例，发现ChatGPT获得的平均分低于人类程序员的平均得分，具体差距在3.9到5.8倍之间，取决于编程语言。文章总结了这些发现，对ChatGPT等基于AI的语言模型的局限性和潜在改进领域提出了关键性见解。
- (5):本文的研究旨在研究ChatGPT在编程解决和问题解决方面的能力，并对ChatGPT的局限性进行深入分析。鉴于ChatGPT在语言处理和理解领域具有极大的潜力和效果，本文意图通过IEEExtreme编程挑战赛的评估来揭示其在编程解决和问题解决方面的能力和限制，并提出未来开发和优化的建议。



